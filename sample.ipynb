{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93656c755b7d75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T16:50:42.040268Z",
     "start_time": "2025-07-27T16:50:38.823260Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load CSV - UPDATE THE PATH BELOW TO YOUR ACTUAL CSV FILE\n",
    "df = pd.read_csv(r\"C:\\Users\\hyazi\\Downloads\\archive (1)\\student dropout.csv\")\n",
    "\n",
    "# 2. Encode categorical columns\n",
    "cat_cols = ['School','Gender','Address','Family_Size','Parental_Status',\n",
    "            'Mother_Job','Father_Job','Reason_for_Choosing_School',\n",
    "            'Guardian','School_Support','Family_Support','Extra_Paid_Class',\n",
    "            'Extra_Curricular_Activities','Attended_Nursery',\n",
    "            'Wants_Higher_Education','Internet_Access','In_Relationship']\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# 3. Separate features and target\n",
    "X = df.drop(['student_id', 'Dropped_Out'], axis=1, errors='ignore')\n",
    "y = df['Dropped_Out'].astype(int)\n",
    "\n",
    "# 4. Split dataset BEFORE applying SMOTE (to avoid leakage)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42)\n",
    "\n",
    "# 5. Apply SMOTE ONLY to training set\n",
    "X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "# 6. Feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 7. Define models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, learning_rate=0.8, algorithm='SAMME', random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, max_depth=4, reg_alpha=0.1, reg_lambda=1,\n",
    "                             eval_metric='logloss', learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# 8. Training & Validation\n",
    "print(\"\\n📊 Validation Results:\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    print(f\"\\n{name} Validation Accuracy: {accuracy_score(y_val, val_pred):.3f}\")\n",
    "    if val_proba is not None:\n",
    "        print(f\"{name} Validation ROC-AUC Score: {roc_auc_score(y_val, val_proba):.3f}\")\n",
    "    print(classification_report(y_val, val_pred))\n",
    "\n",
    "# 9. Final Test Evaluation\n",
    "print(\"\\n🔍 Final Test Set Results:\")\n",
    "for name, model in models.items():\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    print(f\"\\n{name} Test Accuracy: {accuracy_score(y_test, test_pred):.3f}\")\n",
    "    if test_proba is not None:\n",
    "        print(f\"{name} Test ROC-AUC Score: {roc_auc_score(y_test, test_proba):.3f}\")\n",
    "    print(classification_report(y_test, test_pred))\n",
    "\n",
    "    # Optional: Plot Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, test_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed0c073a9c86359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T16:52:02.060556Z",
     "start_time": "2025-07-27T16:52:00.148276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Validation Results:\n",
      "\n",
      "Decision Tree Validation Accuracy: 1.000\n",
      "Decision Tree Validation ROC-AUC Score: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        88\n",
      "           1       1.00      1.00      1.00        88\n",
      "\n",
      "    accuracy                           1.00       176\n",
      "   macro avg       1.00      1.00      1.00       176\n",
      "weighted avg       1.00      1.00      1.00       176\n",
      "\n",
      "\n",
      "SVM Validation Accuracy: 0.966\n",
      "SVM Validation ROC-AUC Score: 0.997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        88\n",
      "           1       0.97      0.97      0.97        88\n",
      "\n",
      "    accuracy                           0.97       176\n",
      "   macro avg       0.97      0.97      0.97       176\n",
      "weighted avg       0.97      0.97      0.97       176\n",
      "\n",
      "\n",
      "Bagging Validation Accuracy: 1.000\n",
      "Bagging Validation ROC-AUC Score: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        88\n",
      "           1       1.00      1.00      1.00        88\n",
      "\n",
      "    accuracy                           1.00       176\n",
      "   macro avg       1.00      1.00      1.00       176\n",
      "weighted avg       1.00      1.00      1.00       176\n",
      "\n",
      "\n",
      "AdaBoost Validation Accuracy: 1.000\n",
      "AdaBoost Validation ROC-AUC Score: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        88\n",
      "           1       1.00      1.00      1.00        88\n",
      "\n",
      "    accuracy                           1.00       176\n",
      "   macro avg       1.00      1.00      1.00       176\n",
      "weighted avg       1.00      1.00      1.00       176\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train,\n\u001b[0;32m     58\u001b[0m                   eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m     59\u001b[0m                   early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     60\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\hyazi\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load Data - UPDATE THE PATH BELOW TO YOUR ACTUAL CSV FILE\n",
    "df = pd.read_csv(r\"C:\\Users\\hyazi\\Downloads\\archive (1)\\student dropout.csv\")\n",
    "\n",
    "# 2. Encode Categorical Features\n",
    "cat_cols = ['School','Gender','Address','Family_Size','Parental_Status',\n",
    "            'Mother_Job','Father_Job','Reason_for_Choosing_School',\n",
    "            'Guardian','School_Support','Family_Support','Extra_Paid_Class',\n",
    "            'Extra_Curricular_Activities','Attended_Nursery',\n",
    "            'Wants_Higher_Education','Internet_Access','In_Relationship']\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# 3. Features and Target\n",
    "X = df.drop(['student_id', 'Dropped_Out'], axis=1, errors='ignore')\n",
    "y = df['Dropped_Out'].astype(int)\n",
    "\n",
    "# 4. Train, Validation, Test Split (Stratified) - BEFORE applying SMOTE\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42)\n",
    "\n",
    "# 5. Apply SMOTE ONLY to training set (to avoid data leakage)\n",
    "X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "# 6. Feature Scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 7. Model Initialization\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, learning_rate=0.8, algorithm='SAMME', random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, max_depth=4, reg_alpha=0.1, reg_lambda=1,\n",
    "                             eval_metric='logloss', learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# 8. Training and Validation\n",
    "print(\"\\n📊 Validation Results:\")\n",
    "for name, model in models.items():\n",
    "    if name == 'XGBoost':\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=False)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    print(f\"\\n{name} Validation Accuracy: {accuracy_score(y_val, val_pred):.3f}\")\n",
    "    if val_proba is not None:\n",
    "        print(f\"{name} Validation ROC-AUC Score: {roc_auc_score(y_val, val_proba):.3f}\")\n",
    "    print(classification_report(y_val, val_pred))\n",
    "\n",
    "# 9. Final Testing\n",
    "print(\"\\n🔍 Final Test Set Results:\")\n",
    "for name, model in models.items():\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    print(f\"\\n{name} Test Accuracy: {accuracy_score(y_test, test_pred):.3f}\")\n",
    "    if test_proba is not None:\n",
    "        print(f\"{name} Test ROC-AUC Score: {roc_auc_score(y_test, test_proba):.3f}\")\n",
    "    print(classification_report(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645f91d57dca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset - UPDATE THE PATH BELOW TO YOUR ACTUAL CSV FILE\n",
    "df = pd.read_csv(r\"C:\\Users\\hyazi\\Downloads\\archive (1)\\student dropout.csv\")\n",
    "\n",
    "# General overview\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df['Dropped_Out'].value_counts())\n",
    "\n",
    "# Set plot theme\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Class Distribution (Dropped Out vs Not)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='Dropped_Out', palette='pastel')\n",
    "plt.title(\"Dropout Class Distribution\")\n",
    "plt.xlabel(\"Dropped Out (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Gender vs Dropout\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='Gender', hue='Dropped_Out', palette='Set2')\n",
    "plt.title(\"Dropout by Gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Family Size vs Dropout\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df, x='Family_Size', hue='Dropped_Out', palette='Set1')\n",
    "plt.title(\"Dropout by Family Size\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Heatmap of Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "encoded_df = df.copy()\n",
    "# Encode categorical columns for correlation heatmap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in encoded_df.select_dtypes(include='object').columns:\n",
    "    encoded_df[col] = LabelEncoder().fit_transform(encoded_df[col])\n",
    "\n",
    "sns.heatmap(encoded_df.corr(), annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Boxplot of Final Grade vs Dropout\n",
    "if 'G3' in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(data=df, x='Dropped_Out', y='G3', palette='Set3')\n",
    "    plt.title(\"Final Grade vs Dropout\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9fd079f6c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's say your full data is in X and y\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ff0fb6672d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE only to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "from collections import Counter\n",
    "print(\"After SMOTE:\", Counter(y_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d92fa9fedce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee63015c8ff397",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,     # prevents deprecation warning\n",
    "    eval_metric='logloss',       # important for classification\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979a339aa8f859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays if not already\n",
    "X_val_np = X_val.values if hasattr(X_val, 'values') else X_val\n",
    "y_val_np = y_val.values if hasattr(y_val, 'values') else y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1561c4ee14cb06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_smote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m      4\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fit using NumPy arrays to avoid TypeError\u001b[39;00m\n\u001b[0;32m     15\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m---> 16\u001b[0m     X_train_smote,\n\u001b[0;32m     17\u001b[0m     y_train_smote,\n\u001b[0;32m     18\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val_np, y_val_np)],\n\u001b[0;32m     19\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     20\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_smote' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit using NumPy arrays to avoid TypeError\n",
    "xgb_model.fit(\n",
    "    X_train_smote,\n",
    "    y_train_smote,\n",
    "    eval_set=[(X_val_np, y_val_np)],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3e8059327698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Example with Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "scores = cross_val_score(dt, X, y, cv=skf, scoring='accuracy')\n",
    "print(\"Decision Tree CV Accuracy: \", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73af6401cb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your data into training and validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val_split, y_val_split)],\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa29652c2f8ba0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59975165bc8438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "print(XGBClassifier.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99545beaabc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(XGBClassifier.fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2b506fd4c74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804db100c1cee062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Dummy dataset\n",
    "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Fit with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cda394e69197a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T11:14:58.662573Z",
     "start_time": "2025-07-27T11:14:53.069280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: xgboost 3.0.2\n",
      "Uninstalling xgboost-3.0.2:\n",
      "  Successfully uninstalled xgboost-3.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hyazi\\anaconda3\\Lib\\site-packages\\~gboost'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall xgboost -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904cf82ea5d129e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
